import re
import sys
import time
import urllib.request
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright, Error as PlaywrightError, TimeoutError as PlaywrightTimeoutError


# GLOBAL USER-AGENT
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"



# ---------------------------------------------------------------------
# âœ”ï¸ GÃœNCEL DOMAIN â€” GitHub TXT DOSYASINDAN (requests yok!)
# ---------------------------------------------------------------------
def find_working_domain(page=None):
    """
    GitHub TXT dosyasÄ±ndan gÃ¼ncel domain'i urllib ile alÄ±r.
    """
    print("\nğŸ” GÃ¼ncel domain GitHub TXT dosyasÄ±ndan alÄ±nÄ±yor...")

    url = "https://raw.githubusercontent.com/koprulu555/selcuk-full-domain-kontrol/main/selcuk_sports_guncel_domain.txt"

    try:
        with urllib.request.urlopen(url, timeout=10) as response:
            domain = response.read().decode().strip()

        if not domain:
            print("âŒ TXT dosyasÄ± boÅŸ!")
            return None

        if not domain.startswith("http"):
            domain = "https://" + domain

        domain = domain.rstrip("/")
        print(f"âœ… GÃ¼ncel domain bulundu: {domain}")
        return domain

    except Exception as e:
        print(f"âŒ Domain okunamadÄ±: {e}")
        return None



# ---------------------------------------------------------------------
# âœ”ï¸ KANAL GRUPLANDIRMA
# ---------------------------------------------------------------------
def get_channel_group(channel_name):
    channel_name_lower = channel_name.lower()
    group_mappings = {
        'BeinSports': ['bein sports', 'beÄ±n sports'],
        'S Sports': ['s sport'],
        'Tivibu': ['tivibu spor', 'tivibu'],
        'Ulusal Kanallar': ['a spor', 'trt spor', 'trt 1', 'tv8', 'atv'],
        'DiÄŸer Spor': ['smart spor', 'nba tv', 'eurosport'],
        'Belgesel': ['national geographic', 'nat geo', 'discovery', 'dmax', 'bbc earth', 'history'],
        'Film & Dizi': ['bein series', 'bein movies', 'movie smart']
    }
    for group, keywords in group_mappings.items():
        for keyword in keywords:
            if keyword in channel_name_lower:
                return group

    if "7/24" in channel_name_lower:
        return "Ulusal Kanallar"

    if not re.search(r'\d{2}:\d{2}', channel_name):
        return "7/24 Kanallar"

    return "MaÃ§ YayÄ±nlarÄ±"



# ---------------------------------------------------------------------
# âœ”ï¸ KANALLARI Ã‡EKME
# ---------------------------------------------------------------------
def scrape_channel_links(page, domain_to_scrape):
    print(f"\nğŸ“¡ Kanallar {domain_to_scrape} adresinden Ã§ekiliyor...")
    channels = []

    try:
        page.goto(domain_to_scrape, timeout=25000, wait_until='domcontentloaded')
        link_elements = page.query_selector_all("a[data-url]")

        if not link_elements:
            print("âŒ 'data-url' iÃ§eren kanal linki bulunamadÄ±.")
            return []

        for link in link_elements:
            player_url = link.get_attribute('data-url')
            name_element = link.query_selector('div.name')

            if name_element and player_url:
                channel_name = name_element.inner_text().strip()

                if player_url.startswith('/'):
                    base_domain = domain_to_scrape.rstrip('/')
                    player_url = f"{base_domain}{player_url}"

                # ORIGIN Ã§ek
                try:
                    parsed_player_url = urlparse(player_url)
                    player_origin = f"{parsed_player_url.scheme}://{parsed_player_url.netloc}"
                except Exception:
                    player_origin = None

                if not player_origin:
                    continue

                # Zaman etiketi
                time_element = link.query_selector('time.time')
                if time_element:
                    t = time_element.inner_text().strip()
                    if t != "7/24":
                        channel_name = f"{channel_name} - {t}"

                group_name = get_channel_group(channel_name)

                channels.append({
                    'name': channel_name,
                    'url': player_url,
                    'group': group_name,
                    'origin': player_origin
                })

        print(f"âœ… {len(channels)} kanal bulundu.")
        return channels

    except PlaywrightError as e:
        print(f"âŒ Kanallar Ã§ekilirken hata: {e}")
        return []



# ---------------------------------------------------------------------
# âœ”ï¸ M3U8 OLUÅTURMA
# ---------------------------------------------------------------------
def extract_m3u8_from_page(page, player_url):
    try:
        page.goto(player_url, timeout=20000, wait_until="domcontentloaded")
        content = page.content()

        base_url_match = re.search(r"this\.baseStreamUrl\s*=\s*['\"](https?://.*?)['\"]", content)
        if not base_url_match:
            print(" -> âŒ baseStreamUrl bulunamadÄ±.", end="")
            return None

        base_url = base_url_match.group(1)

        parsed_url = urlparse(player_url)
        query_params = parse_qs(parsed_url.query)
        stream_id = query_params.get('id', [None])[0]
        if not stream_id:
            print(" -> âŒ 'id' parametresi yok.", end="")
            return None

        return f"{base_url}{stream_id}/playlist.m3u8"

    except Exception:
        print(" -> âŒ Sayfa hatasÄ±.", end="")
        return None



# ---------------------------------------------------------------------
# âœ”ï¸ MAIN
# ---------------------------------------------------------------------
def main():
    with sync_playwright() as p:
        print("ğŸš€ XyzSports M3U8 OluÅŸturucu BaÅŸlatÄ±ldÄ±...")

        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent=USER_AGENT)
        page = context.new_page()

        # DOMAIN TXT'DEN
        xyz_domain = find_working_domain()
        if not xyz_domain:
            print("âŒ Domain bulunamadÄ±, Ã§Ä±kÄ±lÄ±yor.")
            browser.close()
            sys.exit(1)

        channels = scrape_channel_links(page, xyz_domain)
        if not channels:
            print("âŒ Kanal bulunamadÄ±.")
            browser.close()
            sys.exit(1)

        m3u_content = []
        output_filename = "Xyz_srb.m3u"   # âœ”ï¸ SENÄ°N Ä°STEDÄ°ÄÄ°N AD
        created = 0

        # GLOBAL HEADERS
        origin = channels[0]['origin']
        referer = origin + "/"

        m3u_header = [
            "#EXTM3U",
            f"#EXT-X-USER-AGENT:{USER_AGENT}",
            f"#EXT-X-REFERER:{referer}",
            f"#EXT-X-ORIGIN:{origin}"
        ]

        print(f"\nğŸ“º {len(channels)} kanal iÅŸleniyor...\n")

        for i, ch in enumerate(channels, 1):
            print(f"[{i}/{len(channels)}] {ch['name']} (Grup: {ch['group']})...", end="")

            m3u8 = extract_m3u8_from_page(page, ch['url'])

            if m3u8:
                print(" âœ”ï¸")
                m3u_content.append(f'#EXTINF:-1 tvg-name="{ch["name"]}" group-title="{ch["group"]}",{ch["name"]}')
                m3u_content.append(m3u8)
                created += 1
            else:
                print(" âŒ")

        browser.close()

        if created > 0:
            with open(output_filename, "w", encoding="utf-8") as f:
                f.write("\n".join(m3u_header) + "\n\n")
                f.write("\n".join(m3u_content))

            print(f"\nğŸ“‚ {created} kanal '{output_filename}' dosyasÄ±na kaydedildi.")
        else:
            print("\nâ„¹ï¸ HiÃ§bir link oluÅŸturulamadÄ±.")

        print("\nğŸ‰ TamamlandÄ±!")



if __name__ == "__main__":
    main()
